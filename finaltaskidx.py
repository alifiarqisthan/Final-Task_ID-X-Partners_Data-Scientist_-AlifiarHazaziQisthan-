# -*- coding: utf-8 -*-
"""FinalTaskIDX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7NHdxrhp7EZ8XMZPZw21FGhP6pV7BMj

# IMPORT LIBRARIES

Melakukan Import Library dan Framework yang diperlukan dalam segala proses.

NOTE: Beberapa library di-import selain di Cell di bawah ini sesuai kebutuhan kedepannya.
"""

# Commented out IPython magic to ensure Python compatibility.
# import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings("ignore")

"""# Data Understanding

Pada tahap ini, kita akan melakukan analisis dataset yang diberikan untuk memahami struktur dan konten datanya. Dataset ini terdiri dari beberapa kolom yang mencakup berbagai informasi, seperti informasi peminjam, status pinjaman, dan informasi lainnya.


Set maksimal display columns dan rows
```
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 99)
```


"""

data= pd.read_csv("/content/drive/MyDrive/Colab Notebooks/loan_data_2007_2014.csv")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 99)
# check shape of dataset
print("shape of the data:", data.shape)
data.head(5)

data.info()

data.isnull().sum()

"""# Data Cleaning

Pada tahap Data Cleaning ini, dilakukan pembersihan dataset dengan fokus pada penghapusan kolom yang memiliki nilai yang hilang (missing value) atau kolom yang dianggap tidak relevan untuk analisis lanjut. Tujuan utamanya adalah untuk menyederhanakan dataset agar analisis lanjut dapat dilakukan dengan lebih mudah.

"""

data_copy = data.copy()

# Identifikasi kolom yang seluruhnya kosong atau memiliki nilai null
empty_columns = data_copy.columns[data_copy.isnull().all()]
print("Jumlah Kolom yang di drop", len(empty_columns))
print("Kolom yang seluruhnya kosong atau hanya berisi nilai NaN:")
print(empty_columns)
print(data[empty_columns])

# Kolom yang ingin di-drop
columns_to_drop = ['annual_inc_joint', 'dti_joint', 'verification_status_joint',
       'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
       'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
       'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',
       'inq_last_12m']

# Drop kolom yang dipilih
data_copy.drop(columns=columns_to_drop, inplace=True)

data_copy.shape
data_copy.info()

# Print hasil data baru
print("Data after dropping selected columns:")

data_copy.head(20)

for column in data_copy.columns:
    # Print the column name and the number of unique values in that column
    print(f"Column: {column}, Unique Values: {data_copy[column].nunique()}")
    missing_values = data_copy[column].isnull().sum()
    print(f"Jumlah missing value pada kolom {column}: {missing_values}\n")

missing_values = data.isnull().mean()
missing_values[missing_values>0.7]

# Hitung jumlah baris
total_rows = len(data_copy)

# Hitung jumlah baris non-null untuk setiap kolom
nonnull_counts = data_copy.count()

# Hitung persentase data yang terisi untuk setiap kolom
percent_filled = nonnull_counts / total_rows * 100

# Tentukan threshold untuk data yang terisi (misalnya 30%)
threshold = 30

# Filter kolom yang memiliki persentase data terisi di bawah threshold
columns_below_threshold = percent_filled[percent_filled < threshold].index

# Tampilkan kolom-kolom yang memiliki data di bawah 30%
print("Kolom-kolom yang memiliki data di bawah 30%:")
print(columns_below_threshold)

"""Masih pada tahap Data Cleaning, Kali ini akan dilakukan drop untuk kolom-kolom yang tidak perlu dipertahankan untuk membangun sebuah model yang baik dengan kriteria di bawah ini:

1. Merupakan identifiers :
- id, member_id, Unnamed: 0,
2. Hanya memiliki 1 unique value
- policy_code, application_type
3. Merupakan freetext yang tidak penting untuk pemodelan
- url, desc, title  
4. Terlalu banyak memiliki missing value (lebih dari 70%)
- mths_since_last_record, mths_since_last_major_derog
5. Mengandung Data Leakage (Informasi Masa Depan yang tidak bisa mendapatkan data saat melakukan prediksi)
- next_pymnt_d, recoveries, collection_recovery_fee, total_rec_prncp, total_rec_late_fee
6. Mengandung Value yang sama (dengan loan_amnt)
-  funded_amnt, funded_amnt_inv
7. Others
- emp_title, zip_code
"""

data_copy2= data_copy.copy()

identifiers = ["id", "member_id", "Unnamed: 0"]
unique_value_cols = ["policy_code", "application_type"]
freetext_cols = ["url", "desc", "title"]
missing_value_cols = ["mths_since_last_record", "mths_since_last_major_derog"]
data_leakage_cols = ["next_pymnt_d", "recoveries", "collection_recovery_fee", "total_rec_prncp", "total_rec_late_fee"]
same_value_cols = ["funded_amnt", "funded_amnt_inv"]
others_cols = ["emp_title", "zip_code"]

data_copy2.drop(identifiers, axis=1, inplace=True)
data_copy2.drop(unique_value_cols, axis=1, inplace=True)
data_copy2.drop(freetext_cols, axis=1, inplace=True)
data_copy2.drop(missing_value_cols, axis=1, inplace=True)
data_copy2.drop(data_leakage_cols, axis=1, inplace=True)
data_copy2.drop(same_value_cols, axis=1, inplace=True)
data_copy2.drop(others_cols, axis=1, inplace=True)

data_copy2.shape

data_copy2.head()

data_copy2.info()

data_copy2.isnull().sum()

data_copy2.loan_status.value_counts()

data_copy2.shape

data_clean = data_copy2.copy()

data_clean['credit_score'] = np.where(data_clean['loan_status'].isin(['Charged Off','Default','Late (31-120 days)','Late (16-30 days)','Does not meet the credit policy. Status:Charged Off']), "Bad", "Good")

data_clean[['loan_status','credit_score']].head()

data_clean.head(10)

data_clean.shape

import matplotlib.pyplot as plt

# Menghitung jumlah data untuk setiap kategori
counts = data_clean['credit_score'].value_counts()

# Membuat bar chart
plt.figure(figsize=(8, 6))
plt.bar(counts.index, counts.values, color=['green', 'red'])
plt.xlabel('credit_score')
plt.ylabel('Count')
plt.title('Credit Score (GOOD vs BAD)')
plt.show()

data_clean.info()

"""# Exploratory Data Analysis (EDA)
Exploratory Data Analysis (EDA)adalah langkah penting dalam analisis data yang melibatkan pemeriksaan dan visualisasi data untuk memahami karakteristiknya. Pada notebook ini, kita akan melakukan EDA pada dataset untuk mendapatkan wawasan dan mempersiapkan analisis lebih lanjut. Kita akan memulai dengan Analisis Univariat, yang fokusnya pada eksplorasi setiap variabel secara individu. Analisis ini membantu kita memahami distribusi, kecenderungan sentral, dan sebaran variabel numerik dan kategorikal. Setelah itu, kita akan melakukan Analisis Bivariat untuk mengeksplorasi hubungan antar variabel, terutama antara variabel independen dan variabel target. Analisis ini akan membantu kita memahami pengaruh setiap variabel terhadap variabel target. Mari kita telusuri data dan temukan wawasan berharga!

Univariate Analysis
"""

cat_var = data_clean[['term', 'grade', 'sub_grade', 'emp_length', 'home_ownership',
                      'verification_status', 'pymnt_plan',
                      'purpose', 'addr_state', 'initial_list_status']].columns

# Plotting bar chart for each categorical variable
plt.style.use("ggplot")

for column in cat_var:
    plt.figure(figsize=(20, 4))
    plt.subplot(121)
    data[column].value_counts().plot(kind="bar")
    plt.xlabel(column)
    plt.ylabel("number of customers")
    plt.title(column)

plt.style.use("ggplot")
for column in cat_var:
    plt.figure(figsize=(20, 4))
    plt.subplot(121)
    sns.countplot(x=data_clean[column], hue=data_clean["credit_score"])
    plt.title(column)
    plt.xticks(rotation=90)
    plt.show()

# Mengganti kolom dengan tipe data int atau float menjadi numerical_column
numerical_columns = data_clean.select_dtypes(include=['int', 'float']).columns

# Menampilkan hasil
print("Numerical Columns:", numerical_columns)

# Numerical Columns
plt.style.use("ggplot")
for column in numerical_columns:
    average = data[column].mean()
    median = data[column].median()
    mode = data[column].mode()
    std = data[column].std()

    plt.figure(figsize=(20,4))
    plt.subplot(121)
    sns.distplot(data[column], kde=True)
    plt.axvline(average, color='r', linestyle='solid', linewidth=3, label='Mean')
    plt.axvline(median, color='y', linestyle='dotted', linewidth=3, label='Median')
    plt.axvline(mode[0], color='purple', linestyle='dashed', linewidth=3, label='Mode')
    plt.title(column)

    print('Statistic summary of {columns}'.format(columns=column))
    print('Average : ', "%.2f" %average)
    print('Standard deviation : ', "%.2f" %std)
    print('Median : ', "%.2f" %median)
    print('Mode : ', int(mode))

"""# Data Preparation

Modifikasi Tipe Data
"""

data_clean.isna().sum()

data_clean['emp_length'].unique()

data_clean['emp_length'] = data_clean['emp_length'].str.replace('years','').str.replace('year','').str.strip().str.replace('< 1','0.5').str.strip('+')

data_clean['emp_length'] = data_clean['emp_length'].astype(float)
data_clean['emp_length'] = data_clean['emp_length'].replace(np.nan,0)

data_clean['emp_length']

def term_numeric(data_clean, column):
    data_clean[column] = pd.to_numeric(data_clean[column].str.replace(' months', ''))

term_numeric(data_clean, 'term')

data_clean['term']

def date_columns(df, column):
    # store current month
    today_date = pd.to_datetime('2017-01-01')
    # convert to datetime format
    df[column] = pd.to_datetime(df[column], format = "%b-%y")
    # calculate the difference in months and add to a new column
    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
    # make any resulting -ve values to be equal to the max date
    df['mths_since_' + column] = df['mths_since_' + column].apply(lambda x: df['mths_since_' + column].max() if x < 0 else x)
    # drop the original date column
    df.drop(columns = [column], inplace = True)


date_columns(data_clean, 'issue_d')
date_columns(data_clean, 'last_pymnt_d')
date_columns(data_clean, 'last_credit_pull_d')
date_columns(data_clean, 'earliest_cr_line')

data_clean.head()

data_clean.info()

data_clean.isna().sum()

"""Imputasi D"""

data_clean.drop(columns=['mths_since_last_delinq'], inplace=True)

from sklearn.impute import SimpleImputer

# List kolom numerikal dengan missing values
numerical_cols_with_missing = [col for col in data_clean.columns if data_clean[col].dtype in ['int64', 'float64'] and data_clean[col].isnull().any()]

# Inisialisasi SimpleImputer dengan strategi mean
imputer = SimpleImputer(strategy='mean')

# Imputasi missing values dan simpan ke DataFrame
data_clean[numerical_cols_with_missing] = imputer.fit_transform(data_clean[numerical_cols_with_missing])

data_clean.isna().sum()

# Selecting only numerical columns
num_data_clean = data_clean.select_dtypes(include=['int64', 'float64'])

# Calculating the correlation matrix
corr_matrix = num_data_clean.corr()

# Plotting the heatmap
plt.figure(figsize=(20, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", cbar=True)
plt.title('Correlation Heatmap')
plt.show()

print(data_clean[['out_prncp', 'out_prncp_inv']].describe())
print(data_clean[['installment', 'loan_amnt']].describe())

data_clean.drop(columns=['out_prncp_inv'], inplace=True)

data_clean.info()

data_clean.head()

data_clean.describe().T

def detect_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers

# Kolom numerikal yang ingin diperiksa outlier
numerical_cols = [col for col in data_clean.columns if data_clean[col].dtype in ['int64', 'float64']]

# Deteksi outlier untuk setiap kolom numerikal
outliers = data_clean[numerical_cols].apply(detect_outliers)

# Tampilkan jumlah outlier untuk setiap kolom
print(outliers.sum())

data_clean2 = data_clean.copy()

from scipy.stats.mstats import winsorize

# Kolom numerik yang ingin di-Winsorize
numerical_cols = [col for col in data_clean2.columns if data_clean2[col].dtype in ['int64', 'float64']]

# Winsorize data untuk setiap kolom numerik
for col in numerical_cols:
    data_clean2[col] = winsorize(data_clean2[col], limits=[0.2, 0.2])

print(data_clean2)

def detect_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers

# Kolom numerikal yang ingin diperiksa outlier
numerical_cols = [col for col in data_clean2.columns if data_clean2[col].dtype in ['int64', 'float64']]

# Deteksi outlier untuk setiap kolom numerikal
outliers = data_clean2[numerical_cols].apply(detect_outliers)

# Tampilkan jumlah outlier untuk setiap kolom
print(outliers.sum())

data_clean2.drop(columns=['tot_coll_amt'], inplace=True)
data_clean2.drop(columns=['mths_since_last_credit_pull_d'], inplace=True)

data_clean2.info()

numerical_cols = data_clean2.select_dtypes(include=['int64', 'float64']).columns

corr_matrix = data_clean2[numerical_cols].corr().abs()

# Identify columns to drop based on correlation threshold
corr_threshold = 0.8
drop_columns = set()
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > corr_threshold:
            colname = corr_matrix.columns[i]
            drop_columns.add(colname)


# Create a heatmap
plt.figure(figsize=(20, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", cbar=True)
plt.title('Correlation Heatmap')
plt.show()

print("Columns to drop based on Pearson correlation:")
print(drop_columns)

data_clean2.head(20)

data_clean2.drop(columns=['delinq_2yrs'], inplace=True)
data_clean2.drop(columns=['pub_rec'], inplace=True)
data_clean2.drop(columns=['collections_12_mths_ex_med'], inplace=True)
data_clean2.drop(columns=['acc_now_delinq'], inplace=True)
data_clean2.drop(columns=['total_pymnt_inv'], inplace=True)

data_clean2.info()

data_clean2.shape

data_ready= data_clean2.copy()
data_ready.drop_duplicates(inplace=True)

data_ready.shape

data_ready.head()

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()

X = data_ready.drop(['credit_score'], axis=1)
y = data_ready['credit_score']

cat_var= X.select_dtypes(include= ["object"]).columns
print(cat_var)

for column in cat_var:
  X[column] = label.fit_transform(X[column])

X.head()

y.value_counts()

# Handling imbalance data
from imblearn.over_sampling import SMOTE

oversampling = SMOTE(random_state=12, sampling_strategy=1)

# Fit the over sampling
X, y = oversampling.fit_resample(X, y)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 12)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

X_train.head()

X_train_scaled.head()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Inisialisasi model
logreg = LogisticRegression()

# Melatih model menggunakan data latih yang sudah disiapkan
logreg.fit(X_train_scaled, y_train)

# Membuat prediksi menggunakan data uji
y_pred = logreg.predict(X_test_scaled)

# Menampilkan laporan klasifikasi
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix, accuracy_score

# Menghitung confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Menghitung accuracy score
acc_score = accuracy_score(y_test, y_pred)
print("Accuracy Score:", acc_score)

import matplotlib.pyplot as plt
import seaborn as sns

# Membuat confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Membuat heatmap untuk visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Good', 'Bad'],
            yticklabels=['Good', 'Bad'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Log Reg')
plt.show()

from sklearn.tree import DecisionTreeClassifier

# Inisialisasi model Decision Tree
dt_classifier = DecisionTreeClassifier(random_state=12)

# Melatih model menggunakan data latih yang sudah disiapkan
dt_classifier.fit(X_train_scaled, y_train)

# Membuat prediksi menggunakan data uji
y_pred_dt = dt_classifier.predict(X_test_scaled)

# Menampilkan laporan klasifikasi
print("Classification Report for Decision Tree:")
print(classification_report(y_test, y_pred_dt))

# Menghitung confusion matrix
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)

# Membuat heatmap untuk visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Subscribed', 'Subscribed'],
            yticklabels=['Not Subscribed', 'Subscribed'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Decision Tree')
plt.show()

from sklearn.naive_bayes import GaussianNB

# Inisialisasi model Naive Bayes
nb_classifier = GaussianNB()

# Melatih model menggunakan data latih yang sudah disiapkan
nb_classifier.fit(X_train_scaled, y_train)

# Membuat prediksi menggunakan data uji
y_pred_nb = nb_classifier.predict(X_test_scaled)

# Menampilkan laporan klasifikasi
print("Classification Report for Naive Bayes:")
print(classification_report(y_test, y_pred_nb))

# Menghitung confusion matrix
conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)

# Membuat heatmap untuk visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Subscribed', 'Subscribed'],
            yticklabels=['Not Subscribed', 'Subscribed'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Inisialisasi model Random Forest
rf_classifier = RandomForestClassifier(random_state=12)

# Melatih model menggunakan data latih yang sudah disiapkan
rf_classifier.fit(X_train_scaled, y_train)

# Membuat prediksi menggunakan data uji
y_pred_rf = rf_classifier.predict(X_test_scaled)

# Menampilkan laporan klasifikasi
print("Classification Report for Random Forest:")
print(classification_report(y_test, y_pred_rf))

# Menghitung confusion matrix
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

# Membuat heatmap untuk visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Subscribed', 'Subscribed'],
            yticklabels=['Not Subscribed', 'Subscribed'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Random Forest')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

# Inisialisasi model KNN dengan k=5 (default)
knn_classifier = KNeighborsClassifier()

# Melatih model menggunakan data latih yang sudah disiapkan
knn_classifier.fit(X_train_scaled, y_train)

# Membuat prediksi menggunakan data uji
y_pred_knn = knn_classifier.predict(X_test_scaled)

# Menampilkan laporan klasifikasi
print("Classification Report for K-Nearest Neighbors:")
print(classification_report(y_test, y_pred_knn))

# Menghitung confusion matrix
conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)

# Membuat heatmap untuk visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Good', 'Bad'],
            yticklabels=['Good', 'Bad'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - K-Nearest Neighbors')
plt.show()

acc_score = accuracy_score(y_test, y_pred)
print("Accuracy Score:", acc_score)
acc_score_dt = accuracy_score(y_test, y_pred_dt)
print("Accuracy Score:", acc_score_dt)
acc_score_nb = accuracy_score(y_test, y_pred_nb)
print("Accuracy Score:", acc_score_nb)
acc_score_rf = accuracy_score(y_test, y_pred_rf)
print("Accuracy Score:", acc_score_rf)
acc_score_knn= accuracy_score(y_test, y_pred_knn)
print("Accuracy Score:", acc_score_knn)

# Accuracy scores for each model
acc_scores = [acc_score, acc_score_dt, acc_score_nb, acc_score_rf, acc_score_knn]
models = ['Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Random Forest', 'K-Nearest Neighbors']

# Plotting the bar chart
plt.figure(figsize=(10, 6))
plt.bar(models, acc_scores, color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Score Comparison of Different Models')
plt.ylim(0, 1)  # Set the y-axis limit from 0 to 1 for better visualization
plt.show()